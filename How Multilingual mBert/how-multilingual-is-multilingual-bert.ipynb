{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12248326,"sourceType":"datasetVersion","datasetId":7717571}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Investigating the Multilinguality of BERT","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets spacy conllu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:08.877316Z","iopub.execute_input":"2025-07-02T17:58:08.878012Z","iopub.status.idle":"2025-07-02T17:58:12.265250Z","shell.execute_reply.started":"2025-07-02T17:58:08.877968Z","shell.execute_reply":"2025-07-02T17:58:12.264330Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\nRequirement already satisfied: conllu in /usr/local/lib/python3.11/dist-packages (6.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import conllu\n\ndef load_conllu(filename):\n    for sentence in conllu.parse(open(filename, \"rt\", encoding=\"utf-8\").read()):\n        tokenized_words = [token[\"form\"] for token in sentence]\n        gold_tags = [token[\"upos\"] for token in sentence]\n        yield tokenized_words, gold_tags\n        \ncorpus = list(load_conllu(\"/kaggle/input/sequoi-ud-fr/fr_sequoia-ud-test.conllu\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:12.267117Z","iopub.execute_input":"2025-07-02T17:58:12.267488Z","iopub.status.idle":"2025-07-02T17:58:12.453791Z","shell.execute_reply.started":"2025-07-02T17:58:12.267462Z","shell.execute_reply":"2025-07-02T17:58:12.448225Z"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":"## Converting the Conllu file into a DataFrame","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n\ndef conllu2df(corpus:list) -> pd.DataFrame:\n    \n    df = pd.DataFrame(corpus, columns=[\"Words\", \"Tags\"])\n    df[\"SentenceID\"] = df.index\n    df = df.explode([\"Words\", \"Tags\"], ignore_index=True)     # Use explode to flatten each column\n    df = df[[\"SentenceID\", \"Words\", \"Tags\"]]\n    df.columns = [\"sentenceID\", \"token\", \"tag\"]\n    return df ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:12.454774Z","iopub.execute_input":"2025-07-02T17:58:12.455066Z","iopub.status.idle":"2025-07-02T17:58:12.463595Z","shell.execute_reply.started":"2025-07-02T17:58:12.455037Z","shell.execute_reply":"2025-07-02T17:58:12.462843Z"}},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"## Label's Distribution","metadata":{}},{"cell_type":"code","source":"df = conllu2df(corpus)\ndf['tag'].value_counts().plot.bar()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:12.466038Z","iopub.execute_input":"2025-07-02T17:58:12.466612Z","iopub.status.idle":"2025-07-02T17:58:12.696399Z","shell.execute_reply.started":"2025-07-02T17:58:12.466588Z","shell.execute_reply":"2025-07-02T17:58:12.695580Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"<Axes: xlabel='tag'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"# multiword tokens\ndf[df['tag'] == '_']['token'].unique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:12.697182Z","iopub.execute_input":"2025-07-02T17:58:12.697408Z","iopub.status.idle":"2025-07-02T17:58:12.705428Z","shell.execute_reply.started":"2025-07-02T17:58:12.697390Z","shell.execute_reply":"2025-07-02T17:58:12.704551Z"}},"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"array(['des', 'du', 'Aux', 'au', 'desdites', 'Au', 'aux', 'DES'],\n      dtype=object)"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"# The multiword tokens are annotated by ' _ ' \ndf[df['sentenceID'] == 2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:12.706268Z","iopub.execute_input":"2025-07-02T17:58:12.706644Z","iopub.status.idle":"2025-07-02T17:58:12.727676Z","shell.execute_reply.started":"2025-07-02T17:58:12.706620Z","shell.execute_reply":"2025-07-02T17:58:12.726987Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"     sentenceID         token    tag\n80            2            Je   PRON\n81            2            me   PRON\n82            2          pose   VERB\n83            2     également    ADV\n84            2           des    DET\n85            2     questions   NOUN\n86            2         quant    ADV\n87            2             à    ADP\n88            2            la    DET\n89            2       sagesse   NOUN\n90            2            de    ADP\n91            2            l'    DET\n92            2            EU  PROPN\n93            2            et  CCONJ\n94            2            sa    DET\n95            2      réaction   NOUN\n96            2             à    ADP\n97            2            la    DET\n98            2     formation   NOUN\n99            2            du      _\n100           2            de    ADP\n101           2            le    DET\n102           2  gouvernement   NOUN\n103           2            en    ADP\n104           2      Autriche  PROPN\n105           2            et  CCONJ\n106           2            la    DET\n107           2       manière   NOUN\n108           2          dont   PRON\n109           2          cela   PRON\n110           2     affectera   VERB\n111           2            l'    DET\n112           2       opinion   NOUN\n113           2      publique    ADJ\n114           2        là-bas    ADV\n115           2             .  PUNCT","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentenceID</th>\n      <th>token</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>80</th>\n      <td>2</td>\n      <td>Je</td>\n      <td>PRON</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>2</td>\n      <td>me</td>\n      <td>PRON</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>2</td>\n      <td>pose</td>\n      <td>VERB</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>2</td>\n      <td>également</td>\n      <td>ADV</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>2</td>\n      <td>des</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>2</td>\n      <td>questions</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>2</td>\n      <td>quant</td>\n      <td>ADV</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>2</td>\n      <td>à</td>\n      <td>ADP</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>2</td>\n      <td>la</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>2</td>\n      <td>sagesse</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>2</td>\n      <td>de</td>\n      <td>ADP</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>2</td>\n      <td>l'</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>2</td>\n      <td>EU</td>\n      <td>PROPN</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>2</td>\n      <td>et</td>\n      <td>CCONJ</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>2</td>\n      <td>sa</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>2</td>\n      <td>réaction</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>2</td>\n      <td>à</td>\n      <td>ADP</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>2</td>\n      <td>la</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>2</td>\n      <td>formation</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>2</td>\n      <td>du</td>\n      <td>_</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>2</td>\n      <td>de</td>\n      <td>ADP</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>2</td>\n      <td>le</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>102</th>\n      <td>2</td>\n      <td>gouvernement</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>103</th>\n      <td>2</td>\n      <td>en</td>\n      <td>ADP</td>\n    </tr>\n    <tr>\n      <th>104</th>\n      <td>2</td>\n      <td>Autriche</td>\n      <td>PROPN</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>2</td>\n      <td>et</td>\n      <td>CCONJ</td>\n    </tr>\n    <tr>\n      <th>106</th>\n      <td>2</td>\n      <td>la</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>2</td>\n      <td>manière</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>108</th>\n      <td>2</td>\n      <td>dont</td>\n      <td>PRON</td>\n    </tr>\n    <tr>\n      <th>109</th>\n      <td>2</td>\n      <td>cela</td>\n      <td>PRON</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>2</td>\n      <td>affectera</td>\n      <td>VERB</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>2</td>\n      <td>l'</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>2</td>\n      <td>opinion</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>2</td>\n      <td>publique</td>\n      <td>ADJ</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>2</td>\n      <td>là-bas</td>\n      <td>ADV</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>2</td>\n      <td>.</td>\n      <td>PUNCT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"# NUM tags contains Spaces They should be ommited\ndf[df['token'].str.contains(' ')]['tag'].value_counts()\n\n# Removing the spaces from the tokens\ndf['token'] = df['token'].str.replace(' ', '', regex=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:12.728482Z","iopub.execute_input":"2025-07-02T17:58:12.728762Z","iopub.status.idle":"2025-07-02T17:58:12.750058Z","shell.execute_reply.started":"2025-07-02T17:58:12.728739Z","shell.execute_reply":"2025-07-02T17:58:12.749500Z"}},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":"## mBert Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:12.750770Z","iopub.execute_input":"2025-07-02T17:58:12.751035Z","iopub.status.idle":"2025-07-02T17:58:13.174984Z","shell.execute_reply.started":"2025-07-02T17:58:12.750994Z","shell.execute_reply":"2025-07-02T17:58:13.174298Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"UD_tokens = df[df['sentenceID'] == 23]['tag'].to_list()\nprint(UD_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.176278Z","iopub.execute_input":"2025-07-02T17:58:13.176480Z","iopub.status.idle":"2025-07-02T17:58:13.182244Z","shell.execute_reply.started":"2025-07-02T17:58:13.176465Z","shell.execute_reply":"2025-07-02T17:58:13.181262Z"}},"outputs":[{"name":"stdout","text":"['VERB', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'PRON', 'PRON', 'VERB', 'DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', '_', 'ADP', 'DET', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'AUX', 'VERB', '_', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'DET', 'NOUN', 'PUNCT', 'CCONJ', 'ADP', 'PRON', 'PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', '_', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADJ', 'PUNCT']\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"print(\"UD Tokenization:\")\ntokens = df[df['sentenceID'] == 23]['token'].to_list()\nprint(tokens)\nprint(len(tokens))\nprint()\n\n\nprint(\"mBert Tokenization:\")\ntext= \" \".join(df[df['sentenceID'] == 23]['token'].to_list())\nprint(tokenizer.tokenize(text))\nprint(len(tokenizer.tokenize(text)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.185354Z","iopub.execute_input":"2025-07-02T17:58:13.185583Z","iopub.status.idle":"2025-07-02T17:58:13.210958Z","shell.execute_reply.started":"2025-07-02T17:58:13.185567Z","shell.execute_reply":"2025-07-02T17:58:13.210046Z"}},"outputs":[{"name":"stdout","text":"UD Tokenization:\n['Pouvez', '-vous', 'donner', 'les', 'mêmes', 'garanties', 'en', 'ce', 'qui', 'concerne', 'la', 'taxe', 'proposée', 'sur', 'les', 'carburants', 'des', 'de', 'les', 'avions', ',', 'qui', 'a', 'été', 'débattue', 'au', 'à', 'le', 'sein', \"d'\", 'Écofin', 'cette', 'semaine', ',', 'et', 'en', 'ce', 'qui', 'concerne', \"l'\", 'impôt', 'sur', 'les', 'sociétés', 'au', 'à', 'le', 'sein', 'de', \"l'\", 'Union', 'européenne', '?']\n53\n\nmBert Tokenization:\n['Po', '##uve', '##z', '-', 'vous', 'donner', 'les', 'mêmes', 'gara', '##nties', 'en', 'ce', 'qui', 'concerne', 'la', 'tax', '##e', 'proposé', '##e', 'sur', 'les', 'car', '##buran', '##ts', 'des', 'de', 'les', 'avions', ',', 'qui', 'a', 'été', 'débat', '##tue', 'au', 'à', 'le', 'sein', 'd', \"'\", 'É', '##co', '##fin', 'cette', 'semaine', ',', 'et', 'en', 'ce', 'qui', 'concerne', 'l', \"'\", 'im', '##pôt', 'sur', 'les', 'sociétés', 'au', 'à', 'le', 'sein', 'de', 'l', \"'\", 'Union', 'européenne', '?']\n68\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"## Reconciling the Two Tokenizations","metadata":{}},{"cell_type":"code","source":"sentences = df[df['tag'] == '_']['sentenceID']\n\nprint(\"The sentences that contains multiwords\")\nprint(sentences.unique())\n\n\nmultitokens_df = df[df['sentenceID'].isin(sentences.unique())]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.211693Z","iopub.execute_input":"2025-07-02T17:58:13.211929Z","iopub.status.idle":"2025-07-02T17:58:13.230765Z","shell.execute_reply.started":"2025-07-02T17:58:13.211895Z","shell.execute_reply":"2025-07-02T17:58:13.230063Z"}},"outputs":[{"name":"stdout","text":"The sentences that contains multiwords\n[  1   2   3   4   5   8   9  11  14  15  17  19  22  23  25  26  27  28\n  30  31  32  34  41  42  43  46  47  49  53  55  56  57  64  68  70  71\n  73  76  78  86  87  88  89  90  98  99 101 102 103 105 107 108 110 115\n 116 118 119 120 124 127 128 130 131 133 135 137 138 139 141 146 151 155\n 157 160 163 165 166 185 189 191 195 197 199 204 205 212 215 221 229 232\n 235 236 237 238 243 244 248 255 256 257 261 262 263 265 267 268 270 271\n 274 290 291 296 297 298 303 304 305 308 309 311 314 322 323 325 330 332\n 335 337 338 340 341 344 345 348 349 350 354 358 359 360 361 364 365 368\n 369 379 380 381 382 383 384 385 387 388 393 397 398 399 402 404 405 406\n 407 408 409 410 411 416 421 424 425 426 429 433 438 446 447 449 450]\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"tokens  = multitokens_df[multitokens_df['sentenceID'] == 397]['token'].to_list()\ntags    = multitokens_df[multitokens_df['sentenceID'] == 397]['tag'].to_list()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.231810Z","iopub.execute_input":"2025-07-02T17:58:13.232123Z","iopub.status.idle":"2025-07-02T17:58:13.246871Z","shell.execute_reply.started":"2025-07-02T17:58:13.232104Z","shell.execute_reply":"2025-07-02T17:58:13.246277Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"print(tags)\n\n\nlist(zip(tokens,tags))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.247560Z","iopub.execute_input":"2025-07-02T17:58:13.247835Z","iopub.status.idle":"2025-07-02T17:58:13.268629Z","shell.execute_reply.started":"2025-07-02T17:58:13.247813Z","shell.execute_reply":"2025-07-02T17:58:13.267872Z"}},"outputs":[{"name":"stdout","text":"['DET', 'NOUN', 'PROPN', 'PRON', 'AUX', 'VERB', 'DET', 'NUM', 'NOUN', 'NUM', 'PUNCT', 'ADP', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'SCONJ', 'PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', '_', 'ADP', 'DET', 'PROPN', 'PUNCT']\n","output_type":"stream"},{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"[('Le', 'DET'),\n ('Dr', 'NOUN'),\n ('Maréchal', 'PROPN'),\n ('lui', 'PRON'),\n ('aurait', 'AUX'),\n ('proposé', 'VERB'),\n ('le', 'DET'),\n ('11', 'NUM'),\n ('octobre', 'NOUN'),\n ('1994', 'NUM'),\n (',', 'PUNCT'),\n ('de', 'ADP'),\n ('faire', 'VERB'),\n ('pression', 'NOUN'),\n ('sur', 'ADP'),\n ('son', 'DET'),\n ('gendre', 'NOUN'),\n ('pour', 'ADP'),\n (\"qu'\", 'SCONJ'),\n ('il', 'PRON'),\n ('freine', 'VERB'),\n ('son', 'DET'),\n ('instruction', 'NOUN'),\n ('en', 'ADP'),\n ('direction', 'NOUN'),\n ('des', '_'),\n ('de', 'ADP'),\n ('les', 'DET'),\n ('Hauts-de-Seine', 'PROPN'),\n ('.', 'PUNCT')]"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"Root Cause: IamUsing Different LabelEncoders for the Class and the Raw Pipeline","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nimport conllu\n\n\n\nclass ConlluPosDataset:\n    \n\n        \n    def __init__(self, filename: str,tokenizer=None):\n        \n        if tokenizer is None:\n            raise ValueError(\"Please provide a tokenizer\")\n\n        if not isinstance(filename,str):\n            raise ValueError(\"Please provide a valide filename\")\n            \n        self.corpus = list(self._load_conllu(filename))\n        self.df = self._conllu2df(self.corpus)\n        self.all_updated_tokens, self.all_updated_tags = self._update_all_tags_tokens(self.df)\n        \n        self.encoded_inputs = tokenizer(self.all_updated_tokens,\n                                    is_split_into_words=True,\n                                    return_offsets_mapping=True,\n                                    padding=True,\n                                    truncation=True,\n                                    add_special_tokens=False,\n                                )\n        self.all_aligned_tags = self.align_tags_with_subtokens(self.all_updated_tags,self.encoded_inputs)\n        \n        from sklearn.preprocessing import LabelEncoder\n        from itertools import chain\n\n        self.le = LabelEncoder()\n        self.le.fit(list(chain.from_iterable(self.all_aligned_tags)))\n\n\n        self.dataset = self.create_dataset(self.all_aligned_tags, self.encoded_inputs)\n\n        \n\n    def _load_conllu(self, filename):\n        \"\"\"\n        description\n        \"\"\"\n        for sentence in conllu.parse(open(filename, \"rt\", encoding=\"utf-8\").read()):\n            tokenized_words = [token[\"form\"] for token in sentence]\n            gold_tags = [token[\"upos\"] for token in sentence]\n            yield tokenized_words, gold_tags        \n\n    def _conllu2df(self, corpus: list) -> pd.DataFrame:\n        \"\"\"\n        description\n        \"\"\"\n        df = pd.DataFrame(corpus, columns=[\"Words\", \"Tags\"])\n        df[\"SentenceID\"] = df.index\n        df = df.explode([\"Words\", \"Tags\"], ignore_index=True)\n        df = df[[\"SentenceID\", \"Words\", \"Tags\"]]\n        df.columns = [\"sentenceID\", \"token\", \"tag\"]\n        df['token'] = df['token'].str.replace(' ', '', regex=False)\n        return df\n\n    def _update_UD_tokenization(self, tokens, tags):\n        \"\"\"\n        description\n        \"\"\"\n        updated_tokens = []\n        updated_tags = []\n        i = 0\n        while i < len(tags):\n            if tags[i] == '_':\n                merged_tag = tags[i + 1] + '+' + tags[i + 2]\n                updated_tags.append(merged_tag)\n                updated_tokens.append(tokens[i])\n                i += 3\n            else:\n                updated_tags.append(tags[i])\n                updated_tokens.append(tokens[i])\n                i += 1\n        assert len(updated_tags) == len(updated_tokens)\n        return updated_tokens, updated_tags\n\n    def _update_all_tags_tokens(self, corpus_df):\n        all_updated_tags = []\n        all_updated_tokens = []\n\n        for i in corpus_df['sentenceID'].unique():\n            tokens = corpus_df[corpus_df['sentenceID'] == i]['token'].to_list()\n            tags = corpus_df[corpus_df['sentenceID'] == i]['tag'].to_list()\n            updated_tokens, updated_tags = self._update_UD_tokenization(tokens, tags)\n            all_updated_tokens.append(updated_tokens)\n            all_updated_tags.append(updated_tags)\n\n        return all_updated_tokens, all_updated_tags\n\n\n    def align_tags(self,tags: list[str], offset_mapping: list[tuple[int, int]]) -> list[str]:\n        \"\"\"\n        Aligns word-level tags with subword tokens using offset mapping.\n    \n        Parameters:\n            tags (list[str]): A list of word-level tags (e.g., for NER or PoS).\n            offset_mapping (list[tuple[int, int]]): The list of (start, end) character positions\n                                                    for each token produced by the tokenizer.\n    \n        Returns:\n            list[str]: A list of aligned tags, where subword tokens are labeled as '<pad>'\n        \"\"\"\n        tag_iter = iter(tags)\n        aligned_tags = [next(tag_iter)]  # First tag is aligned with the first token\n        prev_end = offset_mapping[0][1]\n    \n        for start, end in offset_mapping[1:]:\n            if start == prev_end:\n                aligned_tags.append('<pad>')  # Subword token\n            else:\n                aligned_tags.append(next(tag_iter, '<pad>'))  # Next word tag or pad\n            prev_end = end\n    \n        assert len(aligned_tags) == len(offset_mapping)\n    \n    \n        return aligned_tags\n\n    def align_tags_with_subtokens(self,all_updated_tags,encoded_inputs):\n        all_aligned_tags = []\n        for index in range(len(all_updated_tags)):\n            tags = all_updated_tags[index]\n            offset_mapping = encoded_inputs['offset_mapping'][index]\n            aligned_tags   = self.align_tags(tags,offset_mapping)\n            all_aligned_tags.append(aligned_tags)\n        return all_aligned_tags\n\n    def get_labels_by_index(self,index, integer_labels = False):\n        if not integer_labels:\n            return self.all_aligned_tags[index]\n        else:\n            return self.le.transform(self.all_aligned_tags[index])\n\n    \n    def create_dataset(self,all_aligned_tags, encoded_inputs):\n        \"\"\"\n        Create a Hugging Face Dataset from aligned tags and tokenized inputs.\n    \n        Parameters:\n            all_aligned_tags (list[list[str]]): The aligned tags per example.\n            encoded_inputs (dict): Tokenizer output with 'input_ids' and 'attention_mask'.\n    \n        Returns:\n            datasets.Dataset: A Hugging Face dataset with input_ids, attention_mask, and labels.\n        \"\"\"\n        data = [\n            {\n                \"input_ids\": input_ids,\n                \"attention_mask\": attention_mask,\n                \"labels\": [-100 if tag == self.le.transform([\"<pad>\"]).item() else tag for tag in self.le.transform(tags).tolist()]\n            }\n            for input_ids, attention_mask, tags in zip(\n                encoded_inputs[\"input_ids\"],\n                encoded_inputs[\"attention_mask\"],\n                all_aligned_tags\n            )\n        ]\n    \n        return Dataset.from_list(data)\n\n\n    def get_dataset(self):\n        return self.dataset\n\n    def number_of_classes(self):\n        return len(self.le.classes_)\n    \n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.269908Z","iopub.execute_input":"2025-07-02T17:58:13.270114Z","iopub.status.idle":"2025-07-02T17:58:13.293038Z","shell.execute_reply.started":"2025-07-02T17:58:13.270099Z","shell.execute_reply":"2025-07-02T17:58:13.292476Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"### UD “updated” tokenziation and UD “updated” labels ","metadata":{}},{"cell_type":"code","source":"def update_UD_tokenization(tokens: list[str], tags: list[str]):\n    updated_tokens = []\n    updated_tags = []\n    i = 0\n    while i < len(tags):\n        if tags[i] == '_':\n            # Merge next two tags\n            merged_tag = tags[i + 1] + '+' + tags[i + 2]\n            updated_tags.append(merged_tag)\n            updated_tokens.append(tokens[i])\n            i += 3  # Skip the next two elements\n        else:\n            updated_tags.append(tags[i])\n            updated_tokens.append(tokens[i])\n            i += 1\n\n    assert len(updated_tags) == len(updated_tokens)\n    return updated_tokens, updated_tags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.293951Z","iopub.execute_input":"2025-07-02T17:58:13.294725Z","iopub.status.idle":"2025-07-02T17:58:13.314201Z","shell.execute_reply.started":"2025-07-02T17:58:13.294703Z","shell.execute_reply":"2025-07-02T17:58:13.313513Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# Testing the function\n\nupdated_tokens, updated_tags = update_UD_tokenization(tokens,tags)\nupdated_tags","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.315096Z","iopub.execute_input":"2025-07-02T17:58:13.315342Z","iopub.status.idle":"2025-07-02T17:58:13.331522Z","shell.execute_reply.started":"2025-07-02T17:58:13.315323Z","shell.execute_reply":"2025-07-02T17:58:13.330881Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"['DET',\n 'NOUN',\n 'PROPN',\n 'PRON',\n 'AUX',\n 'VERB',\n 'DET',\n 'NUM',\n 'NOUN',\n 'NUM',\n 'PUNCT',\n 'ADP',\n 'VERB',\n 'NOUN',\n 'ADP',\n 'DET',\n 'NOUN',\n 'ADP',\n 'SCONJ',\n 'PRON',\n 'VERB',\n 'DET',\n 'NOUN',\n 'ADP',\n 'NOUN',\n 'ADP+DET',\n 'PROPN',\n 'PUNCT']"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.332287Z","iopub.execute_input":"2025-07-02T17:58:13.332505Z","iopub.status.idle":"2025-07-02T17:58:13.354693Z","shell.execute_reply.started":"2025-07-02T17:58:13.332488Z","shell.execute_reply":"2025-07-02T17:58:13.353951Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"       sentenceID         token    tag\n0               0          cela   PRON\n1               0      signifie   VERB\n2               0           que  SCONJ\n3               0          leur    DET\n4               0  consommation   NOUN\n...           ...           ...    ...\n10349         454           les    DET\n10350         454       fraudes   NOUN\n10351         455     Quatrième    ADJ\n10352         455    République   NOUN\n10353         455             .  PUNCT\n\n[10354 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentenceID</th>\n      <th>token</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>cela</td>\n      <td>PRON</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>signifie</td>\n      <td>VERB</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>que</td>\n      <td>SCONJ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>leur</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>consommation</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10349</th>\n      <td>454</td>\n      <td>les</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <th>10350</th>\n      <td>454</td>\n      <td>fraudes</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>10351</th>\n      <td>455</td>\n      <td>Quatrième</td>\n      <td>ADJ</td>\n    </tr>\n    <tr>\n      <th>10352</th>\n      <td>455</td>\n      <td>République</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>10353</th>\n      <td>455</td>\n      <td>.</td>\n      <td>PUNCT</td>\n    </tr>\n  </tbody>\n</table>\n<p>10354 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"import pandas as pd\n\ndef update_tags_tokens(corpus_df) -> dict:\n    all_updated_tags   = []\n    all_updated_tokens = []\n    \n    for i in range(len(corpus_df['sentenceID'].unique())):\n        \n        tokens  = corpus_df[corpus_df['sentenceID'] == i]['token'].to_list()\n        tags    = corpus_df[corpus_df['sentenceID'] == i]['tag'].to_list()\n    \n        updated_tokens, updated_tags = update_UD_tokenization(tokens,tags)\n        \n        all_updated_tags.append(updated_tags)\n        all_updated_tokens.append(updated_tokens)\n        \n    return all_updated_tags, all_updated_tokens\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.355444Z","iopub.execute_input":"2025-07-02T17:58:13.355703Z","iopub.status.idle":"2025-07-02T17:58:13.370523Z","shell.execute_reply.started":"2025-07-02T17:58:13.355684Z","shell.execute_reply":"2025-07-02T17:58:13.369873Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"all_updated_tags, all_updated_tokens =  update_tags_tokens(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.371434Z","iopub.execute_input":"2025-07-02T17:58:13.371781Z","iopub.status.idle":"2025-07-02T17:58:13.632970Z","shell.execute_reply.started":"2025-07-02T17:58:13.371757Z","shell.execute_reply":"2025-07-02T17:58:13.632248Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"index_example = 10\nprint(f\"List of tokens for the {index_example}-th example:\\n{all_updated_tokens[index_example]}\")\nprint(f\"List of tags for the {index_example}-th example:\\n{all_updated_tags[index_example]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.633786Z","iopub.execute_input":"2025-07-02T17:58:13.634136Z","iopub.status.idle":"2025-07-02T17:58:13.638541Z","shell.execute_reply.started":"2025-07-02T17:58:13.634108Z","shell.execute_reply":"2025-07-02T17:58:13.637794Z"}},"outputs":[{"name":"stdout","text":"List of tokens for the 10-th example:\n['Revenons', '-en', 'à', \"l'\", 'actualité', '.']\nList of tags for the 10-th example:\n['VERB', 'PRON', 'ADP', 'DET', 'NOUN', 'PUNCT']\n","output_type":"stream"}],"execution_count":66},{"cell_type":"markdown","source":"### Padding to match the mBert Tokenization","metadata":{}},{"cell_type":"code","source":"texts= ['What a wonderful world!']\nprint(tokenizer.tokenize(\"What a wonderful world!\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.639227Z","iopub.execute_input":"2025-07-02T17:58:13.639380Z","iopub.status.idle":"2025-07-02T17:58:13.659409Z","shell.execute_reply.started":"2025-07-02T17:58:13.639368Z","shell.execute_reply":"2025-07-02T17:58:13.658690Z"}},"outputs":[{"name":"stdout","text":"['What', 'a', 'won', '##der', '##ful', 'world', '!']\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"for l in all_updated_tokens[0:3]:\n    print(l)\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.660116Z","iopub.execute_input":"2025-07-02T17:58:13.660826Z","iopub.status.idle":"2025-07-02T17:58:13.674730Z","shell.execute_reply.started":"2025-07-02T17:58:13.660805Z","shell.execute_reply":"2025-07-02T17:58:13.674075Z"}},"outputs":[{"name":"stdout","text":"['cela', 'signifie', 'que', 'leur', 'consommation', 'énergétique', ',', 'qui', 'représente', 'actuellement', '10', '%', 'de', 'la', 'consommation', 'énergétique', 'moyenne', 'de', \"l'\", 'UE', ',', 'enregistrera', 'une', 'forte', 'augmentation', 'à', 'mesure', \"qu'\", 'ils', 'exigeront', 'des', 'commodités', 'élémentaires', 'comme', \"l'\", 'eau', 'chaude', 'et', ',', 'peut-être', 'même', ',', \"l'\", 'air', 'conditionné', ',', 'des', 'moyens', 'de', 'transport', 'et', 'la', 'modernisation', 'de', 'leurs', 'industries', '.']\n\n['Nous', 'avons', 'noté', 'que', 'la', 'production', \"d'\", 'électricité', 'correspond', 'à', '30', '%', 'des', 'émissions', 'de', 'CO2', 'imputables', 'à', \"l'\", 'homme', '.']\n\n['Je', 'me', 'pose', 'également', 'des', 'questions', 'quant', 'à', 'la', 'sagesse', 'de', \"l'\", 'EU', 'et', 'sa', 'réaction', 'à', 'la', 'formation', 'du', 'gouvernement', 'en', 'Autriche', 'et', 'la', 'manière', 'dont', 'cela', 'affectera', \"l'\", 'opinion', 'publique', 'là-bas', '.']\n\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"example = 100\nencoded_inputs = tokenizer(\n    all_updated_tokens,\n    is_split_into_words=True,\n    return_offsets_mapping=True,\n    padding=True,\n    truncation=True,\n    add_special_tokens=False,\n)\n\n# input the sentences and labels\nprint(\"tokens\")\nprint(all_updated_tokens[example])\nprint('tags')\nprint(all_updated_tags[example])\n\n#print(tokenizer.tokenize(\"Nous ne pouvons nous demander ce qu'il adviendra du Kosovo.\"))\n\n\n#print(t['input_ids'][1:-1])\nprint(encoded_inputs['input_ids'][example])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.675479Z","iopub.execute_input":"2025-07-02T17:58:13.675656Z","iopub.status.idle":"2025-07-02T17:58:13.754407Z","shell.execute_reply.started":"2025-07-02T17:58:13.675643Z","shell.execute_reply":"2025-07-02T17:58:13.753697Z"}},"outputs":[{"name":"stdout","text":"tokens\n['En', 'plein', 'midi', ',', 'la', 'ville', 'de', 'Maxéville', 'sera', 'plongée', 'durant', 'quelques', 'secondes', 'dans', 'la', 'nuit', '.']\ntags\n['ADP', 'ADJ', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'PROPN', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT']\n[10243, 37695, 89663, 117, 10109, 11743, 10104, 12854, 109279, 11270, 16552, 20648, 13891, 11245, 13066, 15533, 57666, 10260, 10109, 26642, 119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"def align_tags(tags: list[str], offset_mapping: list[tuple[int, int]]) -> list[str]:\n    \"\"\"\n    Aligns word-level tags with subword tokens using offset mapping.\n\n    Parameters:\n        tags (list[str]): A list of word-level tags (e.g., for NER or PoS).\n        offset_mapping (list[tuple[int, int]]): The list of (start, end) character positions\n                                                for each token produced by the tokenizer.\n\n    Returns:\n        list[str]: A list of aligned tags, where subword tokens are labeled as '<pad>'\n    \"\"\"\n    tag_iter = iter(tags)\n    aligned_tags = [next(tag_iter)]  # First tag is aligned with the first token\n    prev_end = offset_mapping[0][1]\n\n    for start, end in offset_mapping[1:]:\n        if start == prev_end:\n            aligned_tags.append('<pad>')  # Subword token\n        else:\n            aligned_tags.append(next(tag_iter, '<pad>'))  # Next word tag or pad\n        prev_end = end\n\n    assert len(aligned_tags) == len(offset_mapping)\n\n\n    return aligned_tags\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.755066Z","iopub.execute_input":"2025-07-02T17:58:13.755235Z","iopub.status.idle":"2025-07-02T17:58:13.760153Z","shell.execute_reply.started":"2025-07-02T17:58:13.755221Z","shell.execute_reply":"2025-07-02T17:58:13.759387Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"print(\"Aligning tags with subtokens.....\")\ndef align_tags_with_subtokens(all_updated_tags,encoded_inputs):\n    all_aligned_tags = []\n    for index in range(len(all_updated_tags)):\n        tags = all_updated_tags[index]\n        offset_mapping = encoded_inputs['offset_mapping'][index]\n        aligned_tags   = align_tags(tags,offset_mapping)\n        all_aligned_tags.append(aligned_tags)\n    return all_aligned_tags\n    \nall_aligned_tags = align_tags_with_subtokens(all_updated_tags,encoded_inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.761133Z","iopub.execute_input":"2025-07-02T17:58:13.761386Z","iopub.status.idle":"2025-07-02T17:58:13.786289Z","shell.execute_reply.started":"2025-07-02T17:58:13.761365Z","shell.execute_reply":"2025-07-02T17:58:13.785731Z"}},"outputs":[{"name":"stdout","text":"Aligning tags with subtokens.....\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom itertools import chain\n\n \nle = LabelEncoder()\nle.fit(list(chain.from_iterable(all_aligned_tags)))\n \nle.transform(all_aligned_tags[101])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.787124Z","iopub.execute_input":"2025-07-02T17:58:13.787362Z","iopub.status.idle":"2025-07-02T17:58:13.827567Z","shell.execute_reply.started":"2025-07-02T17:58:13.787341Z","shell.execute_reply":"2025-07-02T17:58:13.827035Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"array([ 2, 15,  0,  7,  8,  2,  7,  8, 12,  7,  8, 15,  4,  3,  8, 11,  0,\n        0,  0,  6,  2,  0,  3,  9,  8, 12,  7,  8, 15,  2,  7, 11, 15, 12,\n        8,  3,  8, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n        0,  0,  0,  0,  0,  0,  0,  0,  0])"},"metadata":{}}],"execution_count":72},{"cell_type":"markdown","source":"## Create Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\ndef create_dataset(all_aligned_tags, encoded_inputs):\n    \"\"\"\n    Create a Hugging Face Dataset from aligned tags and tokenized inputs.\n\n    Parameters:\n        all_aligned_tags (list[list[str]]): The aligned tags per example.\n        encoded_inputs (dict): Tokenizer output with 'input_ids' and 'attention_mask'.\n\n    Returns:\n        datasets.Dataset: A Hugging Face dataset with input_ids, attention_mask, and labels.\n    \"\"\"\n    data = [\n        {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": [-100 if tag == le.transform([\"<pad>\"]).item() else tag for tag in le.transform(tags).tolist()]\n        }\n        for input_ids, attention_mask, tags in zip(\n            encoded_inputs[\"input_ids\"],\n            encoded_inputs[\"attention_mask\"],\n            all_aligned_tags\n        )\n    ]\n\n    return Dataset.from_list(data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.828139Z","iopub.execute_input":"2025-07-02T17:58:13.828318Z","iopub.status.idle":"2025-07-02T17:58:13.832924Z","shell.execute_reply.started":"2025-07-02T17:58:13.828297Z","shell.execute_reply":"2025-07-02T17:58:13.832236Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"train_df = conllu2df(list(load_conllu(\"/kaggle/input/sequoi-ud-fr/fr_sequoia-ud-train.conllu\")))\ntest_df  = conllu2df(list(load_conllu(\"/kaggle/input/sequoi-ud-fr/fr_sequoia-ud-test.conllu\")))\ndev_df   = conllu2df(list(load_conllu(\"/kaggle/input/sequoi-ud-fr/fr_sequoia-ud-dev.conllu\")))\n\ntrain_updated_tags, train_updated_tokens = update_tags_tokens(train_df)\ntest_update_tags, test_updated_tokens    = update_tags_tokens(test_df)\ndev_update_tags, dev_updated_tokens      = update_tags_tokens(dev_df)\n\n\ntrain_encoded_inputs = tokenizer(train_updated_tokens, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, add_special_tokens=False,)\ntest_encoded_inputs  = tokenizer(test_updated_tokens,  is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, add_special_tokens=False,)\ndev_encoded_inputs   = tokenizer(dev_updated_tokens,   is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, add_special_tokens=False,)\n\ntrain_aligned_tags = align_tags_with_subtokens(train_updated_tags,train_encoded_inputs)\ntest_aligned_tags  = align_tags_with_subtokens(test_update_tags,test_encoded_inputs)\ndev_aligned_tags   = align_tags_with_subtokens(dev_update_tags,dev_encoded_inputs)\n\nle = LabelEncoder()\nle.fit(list(chain.from_iterable(train_aligned_tags)))\n\n\ntrain_ds = create_dataset(train_aligned_tags, train_encoded_inputs)\ntest_ds  = create_dataset(test_aligned_tags, test_encoded_inputs)\ndev_ds  = create_dataset(dev_aligned_tags, dev_encoded_inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:13.836205Z","iopub.execute_input":"2025-07-02T17:58:13.836387Z","iopub.status.idle":"2025-07-02T17:58:22.580640Z","shell.execute_reply.started":"2025-07-02T17:58:13.836373Z","shell.execute_reply":"2025-07-02T17:58:22.580117Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = ConlluPosDataset(filename=\"/kaggle/input/sequoi-ud-fr/fr_sequoia-ud-train.conllu\",tokenizer=tokenizer)\ntest  = ConlluPosDataset(filename=\"/kaggle/input/sequoi-ud-fr/fr_sequoia-ud-test.conllu\" ,tokenizer=tokenizer)\ndev   = ConlluPosDataset(filename=\"/kaggle/input/sequoi-ud-fr/fr_sequoia-ud-dev.conllu\", tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:22.581447Z","iopub.execute_input":"2025-07-02T17:58:22.581669Z","iopub.status.idle":"2025-07-02T17:58:48.652504Z","shell.execute_reply.started":"2025-07-02T17:58:22.581651Z","shell.execute_reply":"2025-07-02T17:58:48.651956Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"print(train.get_dataset())\nprint(test.get_dataset())\nprint(dev.get_dataset())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:48.653240Z","iopub.execute_input":"2025-07-02T17:58:48.653470Z","iopub.status.idle":"2025-07-02T17:58:48.657858Z","shell.execute_reply.started":"2025-07-02T17:58:48.653453Z","shell.execute_reply":"2025-07-02T17:58:48.657100Z"}},"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 2231\n})\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 456\n})\nDataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 412\n})\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"PAD_ID = le.transform([\"<pad>\"]).item()\nPAD_ID","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:48.658703Z","iopub.execute_input":"2025-07-02T17:58:48.658952Z","iopub.status.idle":"2025-07-02T17:58:48.678804Z","shell.execute_reply.started":"2025-07-02T17:58:48.658937Z","shell.execute_reply":"2025-07-02T17:58:48.678306Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"print(train.number_of_classes())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:48.679547Z","iopub.execute_input":"2025-07-02T17:58:48.680034Z","iopub.status.idle":"2025-07-02T17:58:48.695324Z","shell.execute_reply.started":"2025-07-02T17:58:48.679990Z","shell.execute_reply":"2025-07-02T17:58:48.694791Z"}},"outputs":[{"name":"stdout","text":"19\n","output_type":"stream"}],"execution_count":78},{"cell_type":"markdown","source":"## Fine-Tuning mBERT","metadata":{}},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:48.695884Z","iopub.execute_input":"2025-07-02T17:58:48.696119Z","iopub.status.idle":"2025-07-02T17:58:48.715612Z","shell.execute_reply.started":"2025-07-02T17:58:48.696103Z","shell.execute_reply":"2025-07-02T17:58:48.715066Z"}},"outputs":[{"name":"stdout","text":"4.51.3\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"batch_size = 16\n\nfrom transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n\nmodel_checkpoint = \"bert-base-multilingual-cased\"\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=19)\nprint(\"Model num_labels:\", model.config.num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T17:58:48.716281Z","iopub.execute_input":"2025-07-02T17:58:48.716500Z","iopub.status.idle":"2025-07-02T17:58:48.874780Z","shell.execute_reply.started":"2025-07-02T17:58:48.716478Z","shell.execute_reply":"2025-07-02T17:58:48.874185Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Model num_labels: 19\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"batch_size = 16\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/finetuned_models\",\n    eval_strategy = \"epoch\",\n    learning_rate=2e-4,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    metric_for_best_model=\"accuracy\",\n    report_to=\"none\",\n    logging_first_step=True,\n\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T18:06:21.905615Z","iopub.execute_input":"2025-07-02T18:06:21.906163Z","iopub.status.idle":"2025-07-02T18:06:21.938588Z","shell.execute_reply.started":"2025-07-02T18:06:21.906138Z","shell.execute_reply":"2025-07-02T18:06:21.937852Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred    \n    preds = np.argmax(predictions,axis=-1)\n\n    # Flatten and mask out -100\n    mask = labels != -100\n    true_labels = labels[mask]\n    true_preds = preds[mask]\n    \n    acc = accuracy_score(true_labels,true_preds)\n\n    return {\n        'accuracy':acc,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T18:06:23.555484Z","iopub.execute_input":"2025-07-02T18:06:23.555963Z","iopub.status.idle":"2025-07-02T18:06:23.560565Z","shell.execute_reply.started":"2025-07-02T18:06:23.555940Z","shell.execute_reply":"2025-07-02T18:06:23.559754Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds, # the train set\n    eval_dataset=dev_ds, # the dev set\n    compute_metrics=compute_metrics\n)\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T18:12:36.679298Z","iopub.execute_input":"2025-07-02T18:12:36.679935Z","iopub.status.idle":"2025-07-02T18:14:33.541784Z","shell.execute_reply.started":"2025-07-02T18:12:36.679913Z","shell.execute_reply":"2025-07-02T18:14:33.541250Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='130' max='130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [130/130 01:55, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.666900</td>\n      <td>0.089539</td>\n      <td>0.978491</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.094200</td>\n      <td>0.065028</td>\n      <td>0.981579</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.081200</td>\n      <td>0.034056</td>\n      <td>0.990635</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.036000</td>\n      <td>0.026483</td>\n      <td>0.991767</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.046400</td>\n      <td>0.018297</td>\n      <td>0.994854</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.022700</td>\n      <td>0.010314</td>\n      <td>0.996810</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.010700</td>\n      <td>0.007802</td>\n      <td>0.998045</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.015500</td>\n      <td>0.005121</td>\n      <td>0.998971</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.007200</td>\n      <td>0.004376</td>\n      <td>0.998868</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.008500</td>\n      <td>0.003186</td>\n      <td>0.999485</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=130, training_loss=0.12790414564884625, metrics={'train_runtime': 116.3353, 'train_samples_per_second': 35.415, 'train_steps_per_second': 1.117, 'total_flos': 342780117138960.0, 'train_loss': 0.12790414564884625, 'epoch': 10.0})"},"metadata":{}}],"execution_count":91},{"cell_type":"code","source":"def accuracy(trainer,test_ds):\n    output = trainer.predict(test_ds)\n    preds = np.argmax(output.predictions, axis=-1)\n    labels = output.label_ids\n\n    # Mask paddingTrainOutput(global_step=350, training_loss=0.2648551310811724, metrics={'train_runtime': 266.3977, 'train_samples_per_second': 41.873, 'train_steps_per_second': 1.314, 'total_flos': 1172917854811380.0, 'train_loss': 0.2648551310811724, 'epoch': 5.0})\n    mask = labels != -100\n    flat_preds = preds[mask]\n    flat_labels = labels[mask]\n\n    return accuracy_score(flat_labels, flat_preds)\n\nacc = accuracy(trainer,test_ds)\nprint(\"Accuracy:\", acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T18:16:16.590516Z","iopub.execute_input":"2025-07-02T18:16:16.591147Z","iopub.status.idle":"2025-07-02T18:16:19.867595Z","shell.execute_reply.started":"2025-07-02T18:16:16.591123Z","shell.execute_reply":"2025-07-02T18:16:19.866966Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.9550030819806863\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"def inference(sentence:str):\n    # tokenization\n\n    # predictions\n\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T18:00:49.273012Z","iopub.execute_input":"2025-07-02T18:00:49.273232Z","iopub.status.idle":"2025-07-02T18:00:50.156983Z","shell.execute_reply.started":"2025-07-02T18:00:49.273215Z","shell.execute_reply":"2025-07-02T18:00:50.156070Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}