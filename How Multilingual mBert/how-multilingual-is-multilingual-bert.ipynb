{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rlLBqNmt-U0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers  datasets spacy conllu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import conllu\n",
        "\n",
        "def load_conllu(filename):\n",
        "    for sentence in conllu.parse(open(filename, \"rt\", encoding=\"utf-8\").read()):\n",
        "        tokenized_words = [token[\"form\"] for token in sentence]\n",
        "        gold_tags = [token[\"upos\"] for token in sentence]\n",
        "        yield tokenized_words, gold_tags\n",
        "\n",
        "corpus = list(load_conllu(\"/kaggle/input/sequoi-ud-fr/fr_sequoia-ud-test.conllu\"))\n"
      ],
      "metadata": {
        "id": "5gaybvNOuBqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def conllu2df(corpus:list) -> pd.DataFrame:\n",
        "\n",
        "    df = pd.DataFrame(corpus, columns=[\"Words\", \"Tags\"])\n",
        "    df[\"SentenceID\"] = df.index\n",
        "    df = df.explode([\"Words\", \"Tags\"], ignore_index=True)     # Use explode to flatten each column\n",
        "    df = df[[\"SentenceID\", \"Words\", \"Tags\"]]\n",
        "    df.columns = [\"sentenceID\", \"token\", \"tag\"]\n",
        "    return df"
      ],
      "metadata": {
        "id": "dtif5F4muDbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = conllu2df(corpus)\n",
        "df['tag'].value_counts().plot.bar()\n"
      ],
      "metadata": {
        "id": "onoQX2-auFsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# multiword tokens\n",
        "df[df['tag'] == '_']['token'].unique()"
      ],
      "metadata": {
        "id": "W41G8EPiuKJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The multiword tokens are annotated by ' _ '\n",
        "df[df['sentenceID'] == 2]"
      ],
      "metadata": {
        "id": "EGUuqEfxuNLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NUM tags contains Spaces They should be ommited\n",
        "df[df['token'].str.contains(' ')]['tag'].value_counts()\n",
        "\n",
        "# Removing the spaces from the tokens\n",
        "df['token'] = df['token'].str.replace(' ', '', regex=False)"
      ],
      "metadata": {
        "id": "D4FolTnLuQmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n"
      ],
      "metadata": {
        "id": "85dSk3OLuTX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UD_tokens = df[df['sentenceID'] == 23]['tag'].to_list()\n",
        "print(UD_tokens)"
      ],
      "metadata": {
        "id": "Gd1lhhzjuWxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"UD Tokenization:\")\n",
        "tokens = df[df['sentenceID'] == 23]['token'].to_list()\n",
        "print(tokens)\n",
        "print(len(tokens))\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"mBert Tokenization:\")\n",
        "text= \" \".join(df[df['sentenceID'] == 23]['token'].to_list())\n",
        "print(tokenizer.tokenize(text))\n",
        "print(len(tokenizer.tokenize(text)))\n"
      ],
      "metadata": {
        "id": "96qBsR_cuaG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df[df['tag'] == '_']['sentenceID']\n",
        "\n",
        "print(\"The sentences that contains multiwords\")\n",
        "print(sentences.unique())\n",
        "\n",
        "\n",
        "multitokens_df = df[df['sentenceID'].isin(sentences.unique())]"
      ],
      "metadata": {
        "id": "sVtHXCBjudwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens  = multitokens_df[multitokens_df['sentenceID'] == 5]['token'].to_list()\n",
        "tags    = multitokens_df[multitokens_df['sentenceID'] == 5]['tag'].to_list()"
      ],
      "metadata": {
        "id": "sX8mHOA5ugSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tags)\n",
        "\n",
        "assert len(tokens) == len(tags)"
      ],
      "metadata": {
        "id": "haghTunJuiOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_UD_tokenization(tokens:list[str], tags:list[str]):\n",
        "    new_tags = []\n",
        "    for i in range(len(tags) -2):\n",
        "        print(i)\n",
        "\n",
        "        if tags[i] == '_':\n",
        "            print(\"index of _ is \",i)\n",
        "            new_tags.append(tags[i+1]+'+'+tags[i+2])\n",
        "\n",
        "\n",
        "            print(new_tags)\n",
        "\n",
        "        else:\n",
        "            new_tags.append(tags[i])\n",
        "\n",
        "\n",
        "        while i <len(tags) -2:\n",
        "            print(tags[i])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aa1OdI2vumxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}